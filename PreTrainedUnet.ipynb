{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pattern Recognition and Machine Learning (PARMA) Group\n",
    "#School of Computing, Costa Rica Institute of Technology\n",
    "#\n",
    "#title           :unet_CellSegmentation.py\n",
    "#description     :Cell segmentation using pretrained unet architecture. \n",
    "#authors         :Willard Zamora wizaca23@gmail.com, \n",
    "#                 Manuel Zumbado manzumbado@ic-itcr.ac.cr\n",
    "\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import glob\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, concatenate, Conv2D, MaxPooling2D, Conv2DTranspose\n",
    "from keras.optimizers import Adam\n",
    "from keras import backend as K\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import ModelCheckpoint,EarlyStopping,TensorBoard,CSVLogger\n",
    "\n",
    "from skimage.io import imread, imshow\n",
    "from skimage import transform\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "HOMEDIR = os.getcwd()\n",
    "BBBCMASK_DIR = '/BBBC_data/mask_creation_images'\n",
    "BATCH_SIZE = 64\n",
    "MAX_EPOCHS = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-13-f21c421f6bf9>, line 17)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-13-f21c421f6bf9>\"\u001b[0;36m, line \u001b[0;32m17\u001b[0m\n\u001b[0;31m    img = img./255\u001b[0m\n\u001b[0m              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "#Compute dice coeficient used in loss function\n",
    "def dice_coef(y_true, y_pred):\n",
    "    #Dice coeficient parameter\n",
    "    smooth = 1\n",
    "    y_true_f = K.flatten(y_true)\n",
    "    y_pred_f = K.flatten(y_pred)\n",
    "    intersection = K.sum(y_true_f * y_pred_f)\n",
    "    return((2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth))\n",
    "\n",
    "#Loss function\n",
    "def dice_coef_loss(y_true, y_pred):\n",
    "    return(-dice_coef(y_true, y_pred))\n",
    "\n",
    "#Process and image\n",
    "def process_img(img_file, shape = (256,256)):\n",
    "    img = imread(img_file)\n",
    "    img = transform.resize(img, shape)\n",
    "    img = img/255\n",
    "\n",
    "    return(img)\n",
    "    \n",
    "    \n",
    "#Define unet architecture\n",
    "def get_unet(img_size = (256,256,1)):\n",
    "    inputs = Input(img_size)\n",
    "    conv1 = Conv2D(32, (3, 3), activation='relu', padding='same')(inputs)\n",
    "    conv1 = Conv2D(32, (3, 3), activation='relu', padding='same')(conv1)\n",
    "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "\n",
    "    conv2 = Conv2D(64, (3, 3), activation='relu', padding='same')(pool1)\n",
    "    conv2 = Conv2D(64, (3, 3), activation='relu', padding='same')(conv2)\n",
    "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "\n",
    "    conv3 = Conv2D(128, (3, 3), activation='relu', padding='same')(pool2)\n",
    "    conv3 = Conv2D(128, (3, 3), activation='relu', padding='same')(conv3)\n",
    "    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
    "\n",
    "    conv4 = Conv2D(256, (3, 3), activation='relu', padding='same')(pool3)\n",
    "    conv4 = Conv2D(256, (3, 3), activation='relu', padding='same')(conv4)\n",
    "    pool4 = MaxPooling2D(pool_size=(2, 2))(conv4)\n",
    "\n",
    "    conv5 = Conv2D(512, (3, 3), activation='relu', padding='same')(pool4)\n",
    "    conv5 = Conv2D(512, (3, 3), activation='relu', padding='same')(conv5)\n",
    "\n",
    "    up6 = concatenate([Conv2DTranspose(256, (2, 2), strides=(2, 2), padding='same')(conv5), conv4], axis=3)\n",
    "    conv6 = Conv2D(256, (3, 3), activation='relu', padding='same')(up6)\n",
    "    conv6 = Conv2D(256, (3, 3), activation='relu', padding='same')(conv6)\n",
    "\n",
    "    up7 = concatenate([Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same')(conv6), conv3], axis=3)\n",
    "    conv7 = Conv2D(128, (3, 3), activation='relu', padding='same')(up7)\n",
    "    conv7 = Conv2D(128, (3, 3), activation='relu', padding='same')(conv7)\n",
    "\n",
    "    up8 = concatenate([Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(conv7), conv2], axis=3)\n",
    "    conv8 = Conv2D(64, (3, 3), activation='relu', padding='same')(up8)\n",
    "    conv8 = Conv2D(64, (3, 3), activation='relu', padding='same')(conv8)\n",
    "\n",
    "    up9 = concatenate([Conv2DTranspose(32, (2, 2), strides=(2, 2), padding='same')(conv8), conv1], axis=3)\n",
    "    conv9 = Conv2D(32, (3, 3), activation='relu', padding='same')(up9)\n",
    "    conv9 = Conv2D(32, (3, 3), activation='relu', padding='same')(conv9)\n",
    "\n",
    "    conv10 = Conv2D(1, (1, 1), activation='sigmoid')(conv9)\n",
    "\n",
    "    model = Model(inputs=[inputs], outputs=[conv10])\n",
    "\n",
    "    model.compile(optimizer=Adam(lr=1e-4), loss=dice_coef_loss, metrics=[dice_coef, 'accuracy'])\n",
    "    #model.summary()\n",
    "    return(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/andy-pohl/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "unet = get_unet()\n",
    "unet.load_weights('./pretrainedUnet/pre_0_3_5.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image_file = HOMEDIR + BBBCMASK_DIR + '/test/image/class0/SIMCEPImages_A01_C1_F1_s01_w2.TIF'\n",
    "test_img = process_img(test_image_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_gen_args = dict(rotation_range = 0,\n",
    "                    width_shift_range=0,\n",
    "                    height_shift_range=0.0,\n",
    "                    zoom_range=0,\n",
    "                    horizontal_flip=False,\n",
    "                    fill_mode='nearest')\n",
    "\n",
    "def trainGenerator(image_folder,mask_folder,aug_dict,batch_size = 32, image_color_mode = \"grayscale\",\n",
    "                    mask_color_mode = \"grayscale\",image_save_prefix  = \"image\",mask_save_prefix  = \"mask\",\n",
    "                   target_size = (256,256),seed = 1):\n",
    "    '''\n",
    "    can generate image and mask at the same time\n",
    "    use the same seed for image_datagen and mask_datagen to ensure the transformation for image and mask is the same\n",
    "    if you want to visualize the results of generator, set save_to_dir = \"your path\"\n",
    "    '''\n",
    "    image_datagen = ImageDataGenerator(**aug_dict)\n",
    "    mask_datagen = ImageDataGenerator(**aug_dict)\n",
    "    image_generator = image_datagen.flow_from_directory(\n",
    "        image_folder,\n",
    "        batch_size = BATCH_SIZE,\n",
    "        color_mode = image_color_mode,\n",
    "        target_size = target_size,\n",
    "        save_prefix  = image_save_prefix,\n",
    "        save_to_dir = None,\n",
    "        class_mode=None,\n",
    "        seed = seed)\n",
    "    mask_generator = mask_datagen.flow_from_directory(\n",
    "        mask_folder,\n",
    "        batch_size = BATCH_SIZE,\n",
    "        color_mode = mask_color_mode,\n",
    "        target_size = target_size,\n",
    "        save_prefix  = mask_save_prefix,\n",
    "        save_to_dir= None,\n",
    "        class_mode=None,\n",
    "        seed = seed)\n",
    "    train_generator = zip(image_generator, mask_generator)\n",
    "    return(train_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 780 images belonging to 1 classes.\n",
      "Found 780 images belonging to 1 classes.\n",
      "Found 50 images belonging to 1 classes.\n",
      "Found 50 images belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "train_image_dir = HOMEDIR + BBBCMASK_DIR + '/train/image/'\n",
    "train_mask_dir = HOMEDIR + BBBCMASK_DIR + '/train/label/'\n",
    "valid_image_dir = HOMEDIR + BBBCMASK_DIR + '/valid/image/'\n",
    "valid_mask_dir = HOMEDIR + BBBCMASK_DIR + '/valid/label/'\n",
    "TrainGen = trainGenerator(train_image_dir, train_mask_dir, data_gen_args)\n",
    "ValidGen = trainGenerator(valid_image_dir, valid_mask_dir, data_gen_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "13/12 [================================] - 260s 20s/step - loss: -1.9668 - dice_coef: 1.9668 - acc: 0.1911 - val_loss: -1.9637 - val_dice_coef: 1.9637 - val_acc: 0.0203\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to -1.96373, saving model to unetCellCnt.hdf5\n",
      "Epoch 2/50\n",
      "13/12 [================================] - 255s 20s/step - loss: -1.9714 - dice_coef: 1.9714 - acc: 0.1846 - val_loss: -1.9786 - val_dice_coef: 1.9786 - val_acc: 0.4272\n",
      "\n",
      "Epoch 00002: val_loss improved from -1.96373 to -1.97864, saving model to unetCellCnt.hdf5\n",
      "Epoch 3/50\n",
      "13/12 [================================] - 254s 20s/step - loss: -1.9831 - dice_coef: 1.9831 - acc: 0.5535 - val_loss: -1.9846 - val_dice_coef: 1.9846 - val_acc: 0.5910\n",
      "\n",
      "Epoch 00003: val_loss improved from -1.97864 to -1.98461, saving model to unetCellCnt.hdf5\n",
      "Epoch 4/50\n",
      "13/12 [================================] - 253s 19s/step - loss: -1.9855 - dice_coef: 1.9855 - acc: 0.6020 - val_loss: -1.9862 - val_dice_coef: 1.9862 - val_acc: 0.6383\n",
      "\n",
      "Epoch 00004: val_loss improved from -1.98461 to -1.98619, saving model to unetCellCnt.hdf5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f1bbc279a58>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_checkpoint = ModelCheckpoint('unetCellCnt_weights.{epoch:02d}-{val_loss:.2f}.hdf5',\n",
    "                                   monitor='val_loss',verbose=1, save_best_only=True)\n",
    "model_earlyStop = EarlyStopping(monitor='val_loss', min_delta = 0.002, patience = 3)\n",
    "model_tensorboard = TensorBoard(log_dir = HOMEDIR + '/mask_tb_logs', write_graph=True, write_images=True)\n",
    "csv_logger = CSVLogger('Mask_training.log')\n",
    "\n",
    "\n",
    "unet.fit_generator(TrainGen,steps_per_epoch=780/BATCH_SIZE,\n",
    "                        validation_data = ValidGen, \n",
    "                        validation_steps = 50/BATCH_SIZE,\n",
    "                        epochs=MAX_EPOCHS,\n",
    "                    callbacks=[model_checkpoint, model_earlyStop, model_tensorboard, csv_logger])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f1b85723d68>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Process and image\n",
    "def process_img(img_file, shape = (256,256)):\n",
    "    img = imread(img_file, plugin='tifffile', as_gray = True)\n",
    "    img = transform.resize(img, shape)\n",
    "    #img = img/255\n",
    "    return(img)\n",
    "test_image_file = HOMEDIR + BBBCMASK_DIR + '/test/image/class0/SIMCEPImages_A18_C74_F1_s08_w1.TIF'\n",
    "test_image_file = '/home/andy-pohl/Documents/ssc_challenge/ssc-case-study-2019/ssc-data/train/A03_C10_F1_s01_w1.TIF'\n",
    "test_img = process_img(test_image_file)\n",
    "imshow(test_img)\n",
    "pred = unet.predict(test_img.reshape(1,256,256,1))\n",
    "imshow(pred.round(0).reshape((256,256)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "imshow(test_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2019-05-17_10:57:27'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
